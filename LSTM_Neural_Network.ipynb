{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8i6IfohP_Nlk"
      },
      "source": [
        "# Spam Classifier\n",
        "\n",
        "## Requirements\n",
        "If you are running this in colab you will need to download the trec07p email database and also 4 text files which define the training and testing sets and their labels. The cells below will download the files from your google drive if they are in a folder called 'spamdata'.  The 4 text files are:\n",
        "\n",
        "* train_set.txt\n",
        "* train_label.txt\n",
        "* test_set.txt\n",
        "* test_label.txt\n",
        "\n",
        "The database is called trec07p.tgz and can be downloaded from:\n",
        "\n",
        "https://plg.uwaterloo.ca/cgi-bin/cgiwrap/gvcormac/trec07p.tgz\n",
        "\n",
        "If you are running this locally the text files should be in the same folder as the python notebook and the trec07p folder should be in a folder called 'Emails'.\n",
        "\n",
        "## Reference\n",
        "Using sections of code from Sijoon Lee, 2019, https://github.com/sijoonlee/spam-ham-walkthrough\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oBsx4muDVE15",
        "outputId": "3506aa21-cf92-4f35-8e29-32fb505db087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K40ovX7WVdWj",
        "colab": {}
      },
      "source": [
        "! cp /content/drive/My\\ Drive/spamdata/trec07p.tgz ./ \n",
        "! tar xvf trec07p.tgz >/dev/null\n",
        "! mkdir Emails\n",
        "! mv trec07p Emails/\n",
        "! cp /content/drive/My\\ Drive/spamdata/*.txt ./\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kurvbE7LZcpt",
        "colab_type": "text"
      },
      "source": [
        "##  Run Options\n",
        "\n",
        "The model has 4 modes:\n",
        "\n",
        "* Use subject only\n",
        "* Use body only\n",
        "* Use body+subject as one sequence\n",
        "* Use body and subject as separate sequences\n",
        "\n",
        "The mode to use can be selected by setting the variable run-selected, e.g.\n",
        "\n",
        "```\n",
        "run_selected = model_type.duallstm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WtXq3aiovvaa",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from time import time\n",
        "from enum import Enum\n",
        "\n",
        "class model_type(Enum):\n",
        "    subject = 1\n",
        "    body = 2\n",
        "    combined = 3\n",
        "    duallstm = 4\n",
        "    \n",
        "# Select the model to run\n",
        "run_selected = model_type.duallstm\n",
        "\n",
        "# Maximum body size.  There are a few outlier very long e-mails which\n",
        "# will cause the training to exceed the memory of the CPU or GPU\n",
        "truncate_body = 5000\n",
        "\n",
        "# Location of the raw dataset\n",
        "email_folder = os.path.join(\"Emails\", \"trec07p\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B9M69GzaRw9",
        "colab_type": "text"
      },
      "source": [
        "## Parsing Emails\n",
        "\n",
        "The function parse_email takes a filename and returns a list of the words in the subject and a list of the words in the body.\n",
        "\n",
        "If the body is base64 encoded it will be decoded and it it is HTML it will be converted to plain text. If there are multiple text and html parts to the body they will be concatenated, although trucated to the value of the truncate_body variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBAf9NO0Oa2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import email\n",
        "import base64\n",
        "from email.parser import Parser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Convert HTML body to plain text\n",
        "def extract_html(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    text = soup.find_all(text=True)\n",
        "\n",
        "    output = ''\n",
        "    blacklist = [\n",
        "    '[document]',\n",
        "    'noscript',\n",
        "    'header',\n",
        "    'html',\n",
        "    'meta',\n",
        "    'head', \n",
        "    'input',\n",
        "    'script',\n",
        "    'style',\n",
        "    ]\n",
        "\n",
        "    for t in text:\n",
        "        if t.parent.name not in blacklist:\n",
        "            output += '{} '.format(t)\n",
        "            \n",
        "    return(output)\n",
        "\n",
        "# Generate a body by combining all parts of the email\n",
        "def walk_email(msg):\n",
        "    body = ''\n",
        "    \n",
        "    for part in msg.walk():\n",
        "        if part.get_content_type() == 'text/plain':\n",
        "            body = body + part.get_payload()\n",
        "        elif part.get_content_type() == 'text/html':\n",
        "# If the body is base64 encoded decode it\n",
        "            if part.get('Content-Transfer-Encoding') == 'base64':\n",
        "                body = body + extract_html(base64.b64decode(part.get_payload()))     \n",
        "            else:\n",
        "                body = body + extract_html(part.get_payload())\n",
        "                \n",
        "        body_words = body.lower().split()\n",
        "        if len(body_words) > truncate_body:\n",
        "            body_words= body_words[:truncate_body]\n",
        "            \n",
        "    return body_words\n",
        "\n",
        "# Process a single email by splitting it into lists of the words in the subject and body\n",
        "def parse_email(filename):\n",
        "    parser = Parser()\n",
        "    with open(filename, encoding='utf-8', errors='replace') as f:\n",
        "        text = f.read()\n",
        "        email = parser.parsestr(text)\n",
        "        subject = email.get('Subject')\n",
        "\n",
        "        body_words = walk_email(email)\n",
        "        \n",
        "        if subject is None:\n",
        "            subject = ''\n",
        "        subject_words = subject.lower().split()\n",
        "        if len(body_words) == 0:\n",
        "            body_words = text.lower().split()\n",
        "            if len(body_words) > truncate_body:\n",
        "                body_words= body_words[:truncate_body]\n",
        "    f.closed\n",
        "    return (subject_words, body_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QqUtZPxOa2b",
        "colab_type": "text"
      },
      "source": [
        "## Read Datasets\n",
        "\n",
        "To ensure the same datasets are used for training and testing the sets are read in from text files.  These should be created in advance and in the same folder as the notebook.\n",
        "\n",
        "The training dataset is further split into a training (70%) and validation (30%) dataset for training the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GwmwN6XUQFHc",
        "outputId": "6e0951bb-fc60-47d6-f210-638a272270b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "\n",
        "print(\"Reading datasets from file:\")\n",
        "f_train = open(\"train_set.txt\",\"r\")\n",
        "f_train_label = open(\"train_label.txt\",\"r\")\n",
        "f_test = open(\"test_set.txt\",\"r\")\n",
        "f_test_label = open(\"test_label.txt\",\"r\")\n",
        "\n",
        "x_train_full = np.loadtxt(f_train,dtype=str,delimiter='\\n')\n",
        "y_train_full = np.loadtxt(f_train_label,dtype=int,delimiter='\\n')\n",
        "x_test = np.loadtxt(f_test,dtype=str,delimiter='\\n')\n",
        "y_test = np.loadtxt(f_test_label,dtype=int,delimiter='\\n')\n",
        "\n",
        "f_train.close()\n",
        "f_train_label.close()\n",
        "f_test.close()\n",
        "f_test_label.close()\n",
        "\n",
        "train_split = 0.7\n",
        "length = len(y_train_full)\n",
        "length_train = int(length*train_split)\n",
        "length_valid = length - length_train\n",
        "\n",
        "x_train = x_train_full[:length_train]\n",
        "y_train = y_train_full[:length_train]\n",
        "x_valid = x_train_full[length_train:]\n",
        "y_valid = y_train_full[length_train:]\n",
        "\n",
        "print(\"Number of emails in training set: %s\" % len(x_train))\n",
        "print(\"Number of emails in validation set: %s\" % len(x_valid))\n",
        "print(\"Number of emails in test set: %s\" % len(x_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading datasets from file:\n",
            "Number of emails in training set: 13125\n",
            "Number of emails in validation set: 5625\n",
            "Number of emails in test set: 6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CJQRu0sOa2f",
        "colab_type": "text"
      },
      "source": [
        "## Build a dictionary\n",
        "\n",
        "Before parsing the emails a dictionary of all the words which occur in the emails is needed.  The first time this notebook is run the entire training set of emails will be read and parsed in order to create the dictionary. Words which occur in fewer than 5 emails will not be included to reduce the size of the dictionary and therefore reduce memory usage.\n",
        "\n",
        "As this is a time consuming process, the dictionary will be saved and used for future runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a38958a4-73e3-4d30-fb48-fab8647ad66c",
        "id": "nrLfHstqOa2g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "cnt = Counter([])\n",
        "\n",
        "if os.path.isfile(\"vocab.dict\"):\n",
        "    print(\"Loading vocab from file...\")\n",
        "    vocab_to_int = pickle.load( open( \"vocab.dict\", \"rb\" ) )\n",
        "else:\n",
        "    print(\"Generating new vocab from email training set...\")\n",
        "    processed = 0\n",
        "    for i in range(len(x_train_full)):\n",
        "        m = os.path.join(email_folder, \"data\", \"inmail.\"+str(x_train_full[i]))\n",
        "\n",
        "        processed = processed + 1\n",
        "        if processed % 500 == 0:\n",
        "            print(\"Building Vocab: %d\" % processed)\n",
        "        email_file = m\n",
        "        \n",
        "        subject_words, body_words = parse_email(email_file)\n",
        "\n",
        "        cnt.update([vocab for vocab in list(dict.fromkeys(subject_words))]) # Remove duplicates so we only count the number of mails with each word\n",
        "        cnt.update([vocab for vocab in list(dict.fromkeys(body_words))])\n",
        "        \n",
        "    vocab_count = cnt.most_common(len(cnt))\n",
        "    vocab_to_int = {word : index+2 for index, (word, count) in enumerate(vocab_count) if count > 5} \n",
        "    vocab_to_int.update({'__PADDING__': 0}) # index 0 for padding\n",
        "    vocab_to_int.update({'__UNKNOWN__': 1}) # index 1 for unknown word such as broken character\n",
        "    pickle.dump( vocab_to_int, open( \"vocab.dict\", \"wb\" ) )\n",
        "    \n",
        "print({k: vocab_to_int[k] for k in list(vocab_to_int.keys())[:20]})\n",
        "print(\"Number of words in vocab: %s\" % len(vocab_to_int))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating new vocab from email training set...\n",
            "Building Vocab: 500\n",
            "Building Vocab: 1000\n",
            "Building Vocab: 1500\n",
            "Building Vocab: 2000\n",
            "Building Vocab: 2500\n",
            "Building Vocab: 3000\n",
            "Building Vocab: 3500\n",
            "Building Vocab: 4000\n",
            "Building Vocab: 4500\n",
            "Building Vocab: 5000\n",
            "Building Vocab: 5500\n",
            "Building Vocab: 6000\n",
            "Building Vocab: 6500\n",
            "Building Vocab: 7000\n",
            "Building Vocab: 7500\n",
            "Building Vocab: 8000\n",
            "Building Vocab: 8500\n",
            "Building Vocab: 9000\n",
            "Building Vocab: 9500\n",
            "Building Vocab: 10000\n",
            "Building Vocab: 10500\n",
            "Building Vocab: 11000\n",
            "Building Vocab: 11500\n",
            "Building Vocab: 12000\n",
            "Building Vocab: 12500\n",
            "Building Vocab: 13000\n",
            "Building Vocab: 13500\n",
            "Building Vocab: 14000\n",
            "Building Vocab: 14500\n",
            "Building Vocab: 15000\n",
            "Building Vocab: 15500\n",
            "Building Vocab: 16000\n",
            "Building Vocab: 16500\n",
            "Building Vocab: 17000\n",
            "Building Vocab: 17500\n",
            "Building Vocab: 18000\n",
            "Building Vocab: 18500\n",
            "{'the': 2, 'to': 3, 'a': 4, 'and': 5, 'for': 6, 'in': 7, 'of': 8, 'you': 9, 'is': 10, 'with': 11, 'your': 12, 'on': 13, 'this': 14, 'that': 15, 'be': 16, 'are': 17, 'have': 18, 'it': 19, 'by': 20, 'not': 21}\n",
            "Number of words in vocab: 36834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVoAVbSiOa2j",
        "colab_type": "text"
      },
      "source": [
        "## Read all emails in dataset\n",
        "\n",
        "Read in all the emails and return a Pandas datafrom containing the tokenised and vectorised subject and body and the spam/ham label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KTIeWQ6iOa2k",
        "colab": {}
      },
      "source": [
        "def parse_emails(x,y):\n",
        "\n",
        "    returned_mail = pd.DataFrame(columns=[\"spam\", \"subject\", \"body\"])\n",
        "    processed = 0\n",
        "    max_subject_length = 0\n",
        "    max_body_length = 0\n",
        "    \n",
        "#     x=x[:2000]\n",
        "    \n",
        "    for i in range(len(x)):\n",
        "        m = os.path.join(email_folder, \"data\", \"inmail.\"+str(x[i]))\n",
        "        l = y[i]\n",
        "\n",
        "        processed = processed + 1\n",
        "        if processed % 500 == 0:\n",
        "            print(\"Parsing emails: %d\" % processed)\n",
        "        spam = l\n",
        "        email_file = m\n",
        "        \n",
        "        subject_words, body_words = parse_email(email_file)\n",
        "\n",
        "        subject_vectors = [ vocab_to_int.get(word, 1) for word in subject_words]\n",
        "        if len(subject_vectors) > max_subject_length:\n",
        "            max_subject_length = len(subject_vectors)\n",
        "\n",
        "        body_vectors = [ vocab_to_int.get(word, 1) for word in body_words]\n",
        "\n",
        "        if (run_selected == model_type.combined):\n",
        "            body_vectors = body_vectors + subject_vectors\n",
        "\n",
        "        if len(body_vectors) > max_body_length:\n",
        "            max_body_length = len(body_vectors)\n",
        "\n",
        "        if len(body_vectors) == 0:\n",
        "            body_vectors = [1]\n",
        "        if len(subject_vectors) == 0:\n",
        "            subject_vectors = [1]\n",
        "        returned_mail = returned_mail.append(pd.DataFrame({\"spam\": spam, \"subject\": [subject_vectors], \"body\": [body_vectors]}), ignore_index = True)\n",
        "\n",
        "    return (returned_mail, max_subject_length, max_body_length)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88WFT2bQOa2n",
        "colab_type": "text"
      },
      "source": [
        "## Create a Custom Data Loader\n",
        "\n",
        "The dataloader takes the dataset and returns an iterator which can be used to read the dataset in batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j4IUDbo0TJr6",
        "colab": {}
      },
      "source": [
        "import torch.utils.data.sampler as splr\n",
        "\n",
        "class CustomDataLoader(object):\n",
        "  def __init__(self, seq_tensor, seq_lengths, body_seq_tensor, body_seq_lengths, label_tensor, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_tensor = seq_tensor\n",
        "    self.seq_lengths = seq_lengths\n",
        "    self.body_seq_tensor = body_seq_tensor\n",
        "    self.body_seq_lengths = body_seq_lengths\n",
        "    self.label_tensor = label_tensor\n",
        "    self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
        "    self.sampler_iter = iter(self.sampler)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
        "    return self\n",
        "\n",
        "  def _next_index(self):\n",
        "    return next(self.sampler_iter) # may raise StopIteration\n",
        "\n",
        "  def __next__(self):\n",
        "    index = self._next_index()\n",
        "\n",
        "    subset_seq_tensor = self.seq_tensor[index].to(device)\n",
        "    subset_seq_lengths = self.seq_lengths[index].to(device)\n",
        "    subset_body_seq_tensor = self.body_seq_tensor[index].to(device)\n",
        "    subset_body_seq_lengths = self.body_seq_lengths[index].to(device)\n",
        "    subset_label_tensor = self.label_tensor[index].to(device)\n",
        "\n",
        "    return subset_seq_tensor, subset_seq_lengths, subset_body_seq_tensor, subset_body_seq_lengths, subset_label_tensor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwZtV-3aOa2r",
        "colab_type": "text"
      },
      "source": [
        "## Generate loaders for each of the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c56d4043-fc63-4060-a4db-5f8cbe0d33cd",
        "id": "_tXdlPUTOa2s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def process_dataset(x,y):\n",
        "    parsed_train, max_subject_length, max_body_length = parse_emails(x,y)\n",
        "    \n",
        "    label_tensor = torch.as_tensor(parsed_train[\"spam\"], dtype = torch.int16)\n",
        "    subject_vectorized_seqs = parsed_train[\"subject\"]\n",
        "    body_vectorized_seqs = parsed_train[\"body\"]\n",
        "\n",
        "    subject_seq_lengths = torch.LongTensor(list(map(len, subject_vectorized_seqs)))\n",
        "    body_seq_lengths = torch.LongTensor(list(map(len, body_vectorized_seqs)))\n",
        "\n",
        "# Pad the sequences because they all need to be the same length\n",
        "    subject_seq_tensor = Variable(torch.zeros((len(subject_vectorized_seqs), max_subject_length))).long()\n",
        "    body_seq_tensor = Variable(torch.zeros((len(body_vectorized_seqs), max_body_length))).long()\n",
        "\n",
        "    for idx, (seq, seqlen) in enumerate(zip(subject_vectorized_seqs, subject_seq_lengths)):\n",
        "      subject_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "    for idx, (seq, seqlen) in enumerate(zip(body_vectorized_seqs, body_seq_lengths)):\n",
        "      body_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "    batch_size = 80\n",
        "    loader = CustomDataLoader(subject_seq_tensor, subject_seq_lengths, body_seq_tensor, body_seq_lengths, label_tensor, batch_size)\n",
        "\n",
        "    return(loader)\n",
        "\n",
        "print(\"Parsing emails for training dataset\")\n",
        "train_loader = process_dataset(x_train, y_train)\n",
        "print(\"Parsing emails for validation dataset\")\n",
        "valid_loader = process_dataset(x_valid, y_valid)\n",
        "print(\"Parsing emails for testing dataset\")\n",
        "test_loader = process_dataset(x_test, y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing emails for training dataset\n",
            "Parsing emails: 500\n",
            "Parsing emails: 1000\n",
            "Parsing emails: 1500\n",
            "Parsing emails: 2000\n",
            "Parsing emails: 2500\n",
            "Parsing emails: 3000\n",
            "Parsing emails: 3500\n",
            "Parsing emails: 4000\n",
            "Parsing emails: 4500\n",
            "Parsing emails: 5000\n",
            "Parsing emails: 5500\n",
            "Parsing emails: 6000\n",
            "Parsing emails: 6500\n",
            "Parsing emails: 7000\n",
            "Parsing emails: 7500\n",
            "Parsing emails: 8000\n",
            "Parsing emails: 8500\n",
            "Parsing emails: 9000\n",
            "Parsing emails: 9500\n",
            "Parsing emails: 10000\n",
            "Parsing emails: 10500\n",
            "Parsing emails: 11000\n",
            "Parsing emails: 11500\n",
            "Parsing emails: 12000\n",
            "Parsing emails: 12500\n",
            "Parsing emails: 13000\n",
            "Parsing emails for validation dataset\n",
            "Parsing emails: 500\n",
            "Parsing emails: 1000\n",
            "Parsing emails: 1500\n",
            "Parsing emails: 2000\n",
            "Parsing emails: 2500\n",
            "Parsing emails: 3000\n",
            "Parsing emails: 3500\n",
            "Parsing emails: 4000\n",
            "Parsing emails: 4500\n",
            "Parsing emails: 5000\n",
            "Parsing emails: 5500\n",
            "Parsing emails for testing dataset\n",
            "Parsing emails: 500\n",
            "Parsing emails: 1000\n",
            "Parsing emails: 1500\n",
            "Parsing emails: 2000\n",
            "Parsing emails: 2500\n",
            "Parsing emails: 3000\n",
            "Parsing emails: 3500\n",
            "Parsing emails: 4000\n",
            "Parsing emails: 4500\n",
            "Parsing emails: 5000\n",
            "Parsing emails: 5500\n",
            "Parsing emails: 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfqAXy6aOa2w",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "The model will be configured to process the subject and body sequences in accordance with the run_selected variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fINdT1zyfhyh",
        "outputId": "b1891c0a-6187-4d2d-86f9-c32bc450eb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# Select the model to run\n",
        "# run_selected = model_type.body\n",
        "\n",
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dimension, fc_dimension, output_size, rnn_layers,\\\n",
        "                 drop_lstm=0.1, drop_out = 0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.fc_dimension = fc_dimension\n",
        "            \n",
        "        # embedding \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dimension)\n",
        "\n",
        "        # LSTM subject layer\n",
        "        self.lstm = nn.LSTM(embedding_dimension, self.fc_dimension, rnn_layers, \n",
        "                            dropout=drop_lstm, batch_first=True)\n",
        "        \n",
        "        # LSTM body layer\n",
        "        self.lstm_body = nn.LSTM(embedding_dimension, self.fc_dimension*2, rnn_layers, \n",
        "                            dropout=drop_lstm, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        \n",
        "        # Fully connected and output layers\n",
        "        if (run_selected == model_type.duallstm):\n",
        "            self.fc = nn.Linear(3*self.fc_dimension, 15)\n",
        "        else:\n",
        "            self.fc = nn.Linear(self.fc_dimension, 15)\n",
        "\n",
        "        self.fc2 = nn.Linear(15, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "        \n",
        "    def one_lstm(self, lstm, x, seq_lengths, fc_dim):\n",
        "        # embeddings\n",
        "        embedded_seq_tensor = self.embedding(x)\n",
        "                \n",
        "        # pack, remove pads\n",
        "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True, enforce_sorted=False)\n",
        "        \n",
        "        # lstm\n",
        "        packed_output, (ht, ct) = lstm(packed_input, None)\n",
        "\n",
        "        # unpack, recover padded sequence\n",
        "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
        "       \n",
        "        # collect the last output in each batch\n",
        "        last_idxs = (input_sizes - 1).to(device) # last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
        "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, fc_dim)).squeeze() # [batch_size, hidden_dim]\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        output = self.dropout(output)\n",
        "        \n",
        "        return(output)\n",
        "\n",
        "    def forward(self, x, seq_lengths, body_x, body_seq_lengths):\n",
        "        \n",
        "        if (run_selected == model_type.duallstm):\n",
        "            subject_output = self.one_lstm(self.lstm, x, seq_lengths, self.fc_dimension)\n",
        "            body_output = self.one_lstm(self.lstm_body, body_x, body_seq_lengths,2*self.fc_dimension)\n",
        "            output= torch.cat((subject_output, body_output), dim=1)\n",
        "        elif (run_selected == model_type.subject):\n",
        "            output = self.one_lstm(self.lstm, x, seq_lengths, self.fc_dimension)\n",
        "        else:\n",
        "            output = self.one_lstm(self.lstm, body_x, body_seq_lengths, self.fc_dimension)\n",
        "            \n",
        "        output = self.fc(output)\n",
        "        output = self.fc2(output).squeeze()\n",
        "               \n",
        "        # sigmoid function\n",
        "        output = self.sig(output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "dict_len = len(vocab_to_int)\n",
        "embedding_dimension = 400\n",
        "fc_dimension = 50\n",
        "output_size = 1\n",
        "rnn_layers = 2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "net = SpamClassifier(dict_len, embedding_dimension, fc_dimension, output_size, rnn_layers, 0.1, 0.1)\n",
        "net = net.to(device)\n",
        "print(net)\n",
        "print(device)\n",
        "\n",
        "# loss and optimization functions\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "lr=0.05\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# After every epoch reduce the learning rate to 30% of its current value\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = 0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpamClassifier(\n",
            "  (embedding): Embedding(36834, 400)\n",
            "  (lstm): LSTM(400, 50, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (lstm_body): LSTM(400, 100, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=150, out_features=15, bias=True)\n",
            "  (fc2): Linear(in_features=15, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DAj1FM9Oa2z",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Train the model on the training dataset. After every 10 batches, run the current model on the validation set and print the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_nj4Bc0n7AZ",
        "outputId": "b0f126c3-6888-4fe7-abb3-15500342e5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import timeit\n",
        "start_time = timeit.default_timer()\n",
        "train_model = True\n",
        "\n",
        "if not train_model:\n",
        "    print (\"Using Saved model...\")\n",
        "    net.load_state_dict(torch.load('spam-pytorch.pth'))\n",
        "else:\n",
        "\n",
        "    print (\"Retraining model...\")\n",
        "    epochs = 4\n",
        "\n",
        "    counter = 0\n",
        "    print_every = 10\n",
        "    clip=5 # gradient clipping\n",
        "\n",
        "    net.train()\n",
        "    # train for some number of epochs\n",
        "    val_losses = []\n",
        "    for e in range(epochs):\n",
        "        for seq_tensor, seq_tensor_lengths, body_seq_tensor, body_seq_tensor_lengths, label in iter(train_loader):\n",
        "            counter += 1\n",
        "    \n",
        "            # get the output from the model\n",
        "            output = net(seq_tensor, seq_tensor_lengths, body_seq_tensor, body_seq_tensor_lengths)\n",
        "        \n",
        "            # get the loss and backprop\n",
        "            loss = criterion(output, label.float())\n",
        "            optimizer.zero_grad() \n",
        "            loss.backward()\n",
        "            \n",
        "            # prevent the exploding gradient\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                \n",
        "                val_losses_in_itr = []\n",
        "                sums = []\n",
        "                sizes = []\n",
        "                \n",
        "                net.eval()\n",
        "                \n",
        "                for seq_tensor, seq_tensor_lengths, body_seq_tensor, body_seq_tensor_lengths, label in iter(valid_loader):\n",
        "                    output = net(seq_tensor, seq_tensor_lengths, body_seq_tensor, body_seq_tensor_lengths)\n",
        "                    \n",
        "                    # losses\n",
        "                    val_loss = criterion(output, label.float())     \n",
        "                    val_losses_in_itr.append(val_loss.item())\n",
        "                    \n",
        "                    # accuracy\n",
        "                    binary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "                    right_or_not = torch.eq(binary_output, label)\n",
        "                    sums.append(torch.sum(right_or_not).float().item())\n",
        "                    sizes.append(right_or_not.shape[0])\n",
        "                \n",
        "                accuracy = sum(sums) / sum(sizes)\n",
        "                \n",
        "                net.train()\n",
        "                print(\"Epoch: {:2d}/{:2d}\\t\".format(e+1, epochs),\n",
        "                      \"Steps: {:3d}\\t\".format(counter),\n",
        "                      \"Loss: {:.4f}\\t\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\\t\".format(np.mean(val_losses_in_itr)),\n",
        "                      \"Accuracy: {:.4f}\".format(accuracy))\n",
        "            scheduler.step(e)\n",
        "            \n",
        "            \n",
        "    training_time = timeit.default_timer() - start_time\n",
        "    print(\"Training Time: {:.1f}\".format(training_time))\n",
        "    print('Saving model...')\n",
        "    torch.save(net.state_dict(), 'spam-pytorch.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Retraining model...\n",
            "Epoch:  1/ 4\t Steps:  10\t Loss: 0.1535\t Val Loss: 0.1438\t Accuracy: 0.9579\n",
            "Epoch:  1/ 4\t Steps:  20\t Loss: 0.0764\t Val Loss: 0.0977\t Accuracy: 0.9705\n",
            "Epoch:  1/ 4\t Steps:  30\t Loss: 0.0889\t Val Loss: 0.0951\t Accuracy: 0.9717\n",
            "Epoch:  1/ 4\t Steps:  40\t Loss: 0.0634\t Val Loss: 0.0991\t Accuracy: 0.9657\n",
            "Epoch:  1/ 4\t Steps:  50\t Loss: 0.0150\t Val Loss: 0.0672\t Accuracy: 0.9796\n",
            "Epoch:  1/ 4\t Steps:  60\t Loss: 0.0328\t Val Loss: 0.0599\t Accuracy: 0.9826\n",
            "Epoch:  1/ 4\t Steps:  70\t Loss: 0.0420\t Val Loss: 0.0561\t Accuracy: 0.9833\n",
            "Epoch:  1/ 4\t Steps:  80\t Loss: 0.0105\t Val Loss: 0.0519\t Accuracy: 0.9870\n",
            "Epoch:  1/ 4\t Steps:  90\t Loss: 0.0377\t Val Loss: 0.0547\t Accuracy: 0.9836\n",
            "Epoch:  1/ 4\t Steps: 100\t Loss: 0.0097\t Val Loss: 0.0523\t Accuracy: 0.9844\n",
            "Epoch:  1/ 4\t Steps: 110\t Loss: 0.1630\t Val Loss: 0.0526\t Accuracy: 0.9845\n",
            "Epoch:  1/ 4\t Steps: 120\t Loss: 0.0082\t Val Loss: 0.0531\t Accuracy: 0.9851\n",
            "Epoch:  1/ 4\t Steps: 130\t Loss: 0.0100\t Val Loss: 0.0686\t Accuracy: 0.9836\n",
            "Epoch:  1/ 4\t Steps: 140\t Loss: 0.0097\t Val Loss: 0.0560\t Accuracy: 0.9842\n",
            "Epoch:  1/ 4\t Steps: 150\t Loss: 0.1729\t Val Loss: 0.0566\t Accuracy: 0.9847\n",
            "Epoch:  1/ 4\t Steps: 160\t Loss: 0.0558\t Val Loss: 0.0516\t Accuracy: 0.9835\n",
            "Epoch:  2/ 4\t Steps: 170\t Loss: 0.0031\t Val Loss: 0.0482\t Accuracy: 0.9863\n",
            "Epoch:  2/ 4\t Steps: 180\t Loss: 0.0138\t Val Loss: 0.0411\t Accuracy: 0.9868\n",
            "Epoch:  2/ 4\t Steps: 190\t Loss: 0.0044\t Val Loss: 0.0372\t Accuracy: 0.9883\n",
            "Epoch:  2/ 4\t Steps: 200\t Loss: 0.0014\t Val Loss: 0.0372\t Accuracy: 0.9888\n",
            "Epoch:  2/ 4\t Steps: 210\t Loss: 0.0014\t Val Loss: 0.0396\t Accuracy: 0.9893\n",
            "Epoch:  2/ 4\t Steps: 220\t Loss: 0.0012\t Val Loss: 0.0432\t Accuracy: 0.9890\n",
            "Epoch:  2/ 4\t Steps: 230\t Loss: 0.0210\t Val Loss: 0.0459\t Accuracy: 0.9860\n",
            "Epoch:  2/ 4\t Steps: 240\t Loss: 0.0111\t Val Loss: 0.0381\t Accuracy: 0.9895\n",
            "Epoch:  2/ 4\t Steps: 250\t Loss: 0.0013\t Val Loss: 0.0373\t Accuracy: 0.9876\n",
            "Epoch:  2/ 4\t Steps: 260\t Loss: 0.0030\t Val Loss: 0.0353\t Accuracy: 0.9904\n",
            "Epoch:  2/ 4\t Steps: 270\t Loss: 0.1155\t Val Loss: 0.0470\t Accuracy: 0.9895\n",
            "Epoch:  2/ 4\t Steps: 280\t Loss: 0.0076\t Val Loss: 0.0481\t Accuracy: 0.9888\n",
            "Epoch:  2/ 4\t Steps: 290\t Loss: 0.0733\t Val Loss: 0.0445\t Accuracy: 0.9876\n",
            "Epoch:  2/ 4\t Steps: 300\t Loss: 0.0121\t Val Loss: 0.0372\t Accuracy: 0.9909\n",
            "Epoch:  2/ 4\t Steps: 310\t Loss: 0.0336\t Val Loss: 0.0341\t Accuracy: 0.9909\n",
            "Epoch:  2/ 4\t Steps: 320\t Loss: 0.0254\t Val Loss: 0.0308\t Accuracy: 0.9916\n",
            "Epoch:  2/ 4\t Steps: 330\t Loss: 0.0020\t Val Loss: 0.0322\t Accuracy: 0.9886\n",
            "Epoch:  3/ 4\t Steps: 340\t Loss: 0.0049\t Val Loss: 0.0307\t Accuracy: 0.9888\n",
            "Epoch:  3/ 4\t Steps: 350\t Loss: 0.0011\t Val Loss: 0.0331\t Accuracy: 0.9893\n",
            "Epoch:  3/ 4\t Steps: 360\t Loss: 0.0016\t Val Loss: 0.0304\t Accuracy: 0.9899\n",
            "Epoch:  3/ 4\t Steps: 370\t Loss: 0.0015\t Val Loss: 0.0307\t Accuracy: 0.9899\n",
            "Epoch:  3/ 4\t Steps: 380\t Loss: 0.0006\t Val Loss: 0.0308\t Accuracy: 0.9899\n",
            "Epoch:  3/ 4\t Steps: 390\t Loss: 0.0031\t Val Loss: 0.0312\t Accuracy: 0.9892\n",
            "Epoch:  3/ 4\t Steps: 400\t Loss: 0.0002\t Val Loss: 0.0320\t Accuracy: 0.9895\n",
            "Epoch:  3/ 4\t Steps: 410\t Loss: 0.0037\t Val Loss: 0.0318\t Accuracy: 0.9899\n",
            "Epoch:  3/ 4\t Steps: 420\t Loss: 0.0002\t Val Loss: 0.0318\t Accuracy: 0.9899\n",
            "Epoch:  3/ 4\t Steps: 430\t Loss: 0.0001\t Val Loss: 0.0316\t Accuracy: 0.9904\n",
            "Epoch:  3/ 4\t Steps: 440\t Loss: 0.0007\t Val Loss: 0.0314\t Accuracy: 0.9899\n",
            "Epoch:  3/ 4\t Steps: 450\t Loss: 0.0027\t Val Loss: 0.0317\t Accuracy: 0.9902\n",
            "Epoch:  3/ 4\t Steps: 460\t Loss: 0.0350\t Val Loss: 0.0316\t Accuracy: 0.9908\n",
            "Epoch:  3/ 4\t Steps: 470\t Loss: 0.0021\t Val Loss: 0.0347\t Accuracy: 0.9888\n",
            "Epoch:  3/ 4\t Steps: 480\t Loss: 0.0036\t Val Loss: 0.0340\t Accuracy: 0.9892\n",
            "Epoch:  3/ 4\t Steps: 490\t Loss: 0.0005\t Val Loss: 0.0320\t Accuracy: 0.9893\n",
            "Epoch:  4/ 4\t Steps: 500\t Loss: 0.0022\t Val Loss: 0.0312\t Accuracy: 0.9900\n",
            "Epoch:  4/ 4\t Steps: 510\t Loss: 0.0006\t Val Loss: 0.0314\t Accuracy: 0.9899\n",
            "Epoch:  4/ 4\t Steps: 520\t Loss: 0.0030\t Val Loss: 0.0310\t Accuracy: 0.9902\n",
            "Epoch:  4/ 4\t Steps: 530\t Loss: 0.0029\t Val Loss: 0.0307\t Accuracy: 0.9904\n",
            "Epoch:  4/ 4\t Steps: 540\t Loss: 0.0006\t Val Loss: 0.0308\t Accuracy: 0.9904\n",
            "Epoch:  4/ 4\t Steps: 550\t Loss: 0.0006\t Val Loss: 0.0309\t Accuracy: 0.9906\n",
            "Epoch:  4/ 4\t Steps: 560\t Loss: 0.0002\t Val Loss: 0.0307\t Accuracy: 0.9908\n",
            "Epoch:  4/ 4\t Steps: 570\t Loss: 0.0009\t Val Loss: 0.0306\t Accuracy: 0.9909\n",
            "Epoch:  4/ 4\t Steps: 580\t Loss: 0.0002\t Val Loss: 0.0306\t Accuracy: 0.9909\n",
            "Epoch:  4/ 4\t Steps: 590\t Loss: 0.0006\t Val Loss: 0.0306\t Accuracy: 0.9909\n",
            "Epoch:  4/ 4\t Steps: 600\t Loss: 0.0007\t Val Loss: 0.0316\t Accuracy: 0.9913\n",
            "Epoch:  4/ 4\t Steps: 610\t Loss: 0.0004\t Val Loss: 0.0303\t Accuracy: 0.9913\n",
            "Epoch:  4/ 4\t Steps: 620\t Loss: 0.0006\t Val Loss: 0.0306\t Accuracy: 0.9915\n",
            "Epoch:  4/ 4\t Steps: 630\t Loss: 0.0007\t Val Loss: 0.0301\t Accuracy: 0.9916\n",
            "Epoch:  4/ 4\t Steps: 640\t Loss: 0.0001\t Val Loss: 0.0352\t Accuracy: 0.9916\n",
            "Epoch:  4/ 4\t Steps: 650\t Loss: 0.0003\t Val Loss: 0.0299\t Accuracy: 0.9915\n",
            "Epoch:  4/ 4\t Steps: 660\t Loss: 0.0114\t Val Loss: 0.0299\t Accuracy: 0.9913\n",
            "Training Time: 1614.2\n",
            "Saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcYTqt7oOa23",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "\n",
        "Run the model on the test set and print the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hdf_311Xe7Di",
        "outputId": "766518ae-85fd-41de-9abb-bc4f419f527f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score,accuracy_score,roc_curve,auc,roc_auc_score\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "test_losses = []\n",
        "sums = []\n",
        "sizes = []\n",
        "\n",
        "net.eval()\n",
        "\n",
        "# Increasing threshold makes false positives more likely\n",
        "threshold = 0.5\n",
        "\n",
        "total_output = []\n",
        "total_results = []\n",
        "total_labels = []\n",
        "for seq_tensor, seq_tensor_lengths, body_seq_tensor, body_seq_tensor_lengths, label in iter(test_loader):\n",
        "\n",
        "    output = net(seq_tensor, seq_tensor_lengths, body_seq_tensor, body_seq_tensor_lengths).cpu().detach().numpy()\n",
        "    total_output = np.hstack((total_output, output))\n",
        "    result = np.round(output+(threshold-0.5), 0)\n",
        "\n",
        "    total_results = np.hstack((total_results, result))\n",
        "    total_labels = np.hstack((total_labels, label.cpu()))\n",
        "\n",
        "    \n",
        "%matplotlib inline\n",
        "from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score,accuracy_score,roc_curve,auc,roc_auc_score\n",
        "auc1= roc_auc_score(total_labels, total_results)\n",
        "\n",
        "fpr, tpr, threshold = roc_curve(total_labels, total_results,pos_label=1)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, color = 'blue', label = 'AUC = %0.3f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],linestyle='--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()    \n",
        "    \n",
        "y_actu = pd.Series(total_labels, name='Actual')\n",
        "y_pred = pd.Series(total_results, name='Predicted')\n",
        "df_confusion = pd.crosstab(y_actu, y_pred)\n",
        "print(df_confusion)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(total_labels, total_results).ravel()\n",
        "\n",
        "print(\" \")\n",
        "print(\"True Negative: \",tn)\n",
        "print(\"False Positive: \",fp)\n",
        "print(\"False Negative: \",fn)\n",
        "print(\"True Positive: \",tp)\n",
        "print(\" \")\n",
        "print(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(total_labels, total_results)))\n",
        "print(\"F1 Score: {:.2f}%\".format(100 * f1_score(total_labels, total_results)))\n",
        "print(\"Precision: {:.2f}%\".format(100 * precision_score(total_labels, total_results)))\n",
        "print(\"Recall: {:.2f}%\".format(100 * recall_score(total_labels, total_results)))\n",
        "print(\" \")\n",
        "print('AUC: %.3f' % auc1)\n",
        "print(\" \")\n",
        "print(\"Training Time: {:.1f}\".format(training_time))\n",
        "print(\"Testing Time: {:.1f}\".format(timeit.default_timer() - start_time))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted   0.0   1.0\n",
            "Actual               \n",
            "0.0        2091    34\n",
            "1.0          29  4096\n",
            " \n",
            "True Negative:  2091\n",
            "False Positive:  34\n",
            "False Negative:  29\n",
            "True Positive:  4096\n",
            " \n",
            "Accuracy: 98.99%\n",
            "F1 Score: 99.24%\n",
            "Precision: 99.18%\n",
            "Recall: 99.30%\n",
            " \n",
            "AUC: 0.988\n",
            " \n",
            "Training Time: 1614.2\n",
            "Testing Time: 17.3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}